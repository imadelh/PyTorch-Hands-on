{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Variables-AutomaticDifferentiation.ipynb","version":"0.3.2","views":{},"default_view":{},"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"yvQz50Kk9LGO","colab_type":"text"},"cell_type":"markdown","source":["# Automatic differentiation\n"]},{"metadata":{"id":"8TVyS_J49MR5","colab_type":"text"},"cell_type":"markdown","source":["PyTorch records all computations to be able to backpropagate through them. That is, provided a sequence of operations that starts from a tensor $\\theta$ to define a scalar $g(\\theta)$, it is able to compute \n","$\\nabla_\\theta g(\\theta)$ exactly, with only one function call.\n","\n","The **autograd** package allows automatic differentiation for all operations on Tensors.\n","\n","*autograd.Variable* is the main class of the package it has 3 attributes :\n","  - .data : contains the data (tensor) stored in a variable\n","  - .grad : gradient w.r.t. this variable\n","  - .grad_fn : contains the *Function* that has created the variable (for user's variables = None)"]},{"metadata":{"id":"HskGWeQF_Ta2","colab_type":"text"},"cell_type":"markdown","source":["Let's test this on a simple example"]},{"metadata":{"id":"KNG7dOk1nTw-","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":306},"outputId":"cacc0ca3-b23b-4198-fe30-e2554a5901f7","executionInfo":{"status":"ok","timestamp":1523012770138,"user_tz":-120,"elapsed":77162,"user":{"displayName":"Imad El Hanafi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"100919786130867571645"}}},"cell_type":"code","source":["!pip3 install torch torchvision"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Collecting torch\n","  Downloading torch-0.3.1-cp36-cp36m-manylinux1_x86_64.whl (496.4MB)\n","\u001b[K    100% |████████████████████████████████| 496.4MB 2.7kB/s \n","\u001b[?25hCollecting torchvision\n","  Downloading torchvision-0.2.0-py2.py3-none-any.whl (48kB)\n","\u001b[K    100% |████████████████████████████████| 51kB 8.6MB/s \n","\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from torch)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch)\n","Collecting pillow>=4.1.1 (from torchvision)\n","  Downloading Pillow-5.1.0-cp36-cp36m-manylinux1_x86_64.whl (2.0MB)\n","\u001b[K    100% |████████████████████████████████| 2.0MB 651kB/s \n","\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision)\n","Installing collected packages: torch, pillow, torchvision\n","  Found existing installation: Pillow 4.0.0\n","    Uninstalling Pillow-4.0.0:\n","      Successfully uninstalled Pillow-4.0.0\n","Successfully installed pillow-5.1.0 torch-0.3.1 torchvision-0.2.0\n"],"name":"stdout"}]},{"metadata":{"id":"J3Ewiwk_9yUZ","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["import torch as th\n","from torch.autograd import Variable\n","\n","## Define a function f\n","\n","def f(x):\n","    return th.sqrt(th.sum(th.pow(x,2), dim=0))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"JzELfid1_R0F","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# Define a point x on a GPU\n","\n","dtype = th.cuda.FloatTensor\n","n = 10\n","x = th.randn(n).type(dtype)\n","\n","# or directly : \n","# x = th.randn(n).cuda()\n","\n","### Define a variable \n","\n","x = Variable(x, requires_grad=True)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"U8Tjl0l9AjYL","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":272},"outputId":"76d8a951-f797-46f7-bdef-5e8268538520","executionInfo":{"status":"ok","timestamp":1522968796319,"user_tz":-120,"elapsed":468,"user":{"displayName":"Imad El Hanafi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"100919786130867571645"}}},"cell_type":"code","source":["# Attributes of a variable\n","\n","print(x.grad)\n","print(x.grad_fn)\n","print(x.data)"],"execution_count":12,"outputs":[{"output_type":"stream","text":["None\n","None\n","\n"," 0.4928\n"," 1.9446\n"," 1.1037\n"," 1.8665\n"," 0.0570\n","-0.0988\n"," 0.6488\n","-0.6188\n","-1.7103\n","-0.3091\n","[torch.cuda.FloatTensor of size 10 (GPU 0)]\n","\n"],"name":"stdout"}]},{"metadata":{"id":"cVe7p_q2BIlP","colab_type":"text"},"cell_type":"markdown","source":["Now we can compte f(x) "]},{"metadata":{"id":"8xllsoTvAwJ6","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":85},"outputId":"b5a51455-6a5b-4f82-c2da-692353524712","executionInfo":{"status":"ok","timestamp":1522969294006,"user_tz":-120,"elapsed":430,"user":{"displayName":"Imad El Hanafi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"100919786130867571645"}}},"cell_type":"code","source":["out = f(x)\n","print(out)"],"execution_count":33,"outputs":[{"output_type":"stream","text":["Variable containing:\n"," 3.5446\n","[torch.cuda.FloatTensor of size 1 (GPU 0)]\n","\n"],"name":"stdout"}]},{"metadata":{"id":"__mhRaFsBSFk","colab_type":"text"},"cell_type":"markdown","source":["After calculating the output, we can do a backprop and get the gradiant w.r.t. x"]},{"metadata":{"id":"busqAYy_BFkV","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["out.backward(create_graph = True) # creat graph useful for higher order derivative products"],"execution_count":0,"outputs":[]},{"metadata":{"id":"eqEa22wEBGLF","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":255},"outputId":"fe0c02b2-a146-49ee-d05c-40c34aaaa7a7","executionInfo":{"status":"ok","timestamp":1522969296913,"user_tz":-120,"elapsed":439,"user":{"displayName":"Imad El Hanafi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"100919786130867571645"}}},"cell_type":"code","source":["print(x.grad)\n","print(out.grad_fn)"],"execution_count":35,"outputs":[{"output_type":"stream","text":["Variable containing:\n"," 0.1390\n"," 0.5486\n"," 0.3114\n"," 0.5266\n"," 0.0161\n","-0.0279\n"," 0.1830\n","-0.1746\n","-0.4825\n","-0.0872\n","[torch.cuda.FloatTensor of size 10 (GPU 0)]\n","\n","<SqrtBackward object at 0x7fc007c22ac8>\n"],"name":"stdout"}]},{"metadata":{"id":"fLO3sssiB7_M","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":221},"outputId":"ee831904-2aee-4e23-9081-e30825f3fb00","executionInfo":{"status":"ok","timestamp":1522969299552,"user_tz":-120,"elapsed":513,"user":{"displayName":"Imad El Hanafi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"100919786130867571645"}}},"cell_type":"code","source":["# We can reset the data in the grad to zero\n","\n","x.grad.data.zero_()"],"execution_count":36,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\n"," 0\n"," 0\n"," 0\n"," 0\n"," 0\n"," 0\n"," 0\n"," 0\n"," 0\n"," 0\n","[torch.cuda.FloatTensor of size 10 (GPU 0)]"]},"metadata":{"tags":[]},"execution_count":36}]},{"metadata":{"id":"2nxN3G4VCeiG","colab_type":"text"},"cell_type":"markdown","source":["We can also work with multivariables functions"]},{"metadata":{"id":"0Y9laNitCJwh","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["## Define a function\n","\n","def g(x,y,z):\n","  return (th.log(x**2 + y**2) + z*x*y).mean()\n","\n","\n","## Define variables : Only x and y will be considered as variables, z is going to be a pramater\n","\n","x = Variable(th.randn(n).cuda(), requires_grad=True)\n","y = Variable(th.randn(n).cuda(), requires_grad=True)\n","z = Variable(th.randn(n).cuda(), requires_grad=False)\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Av0JAx4rDWPY","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":68},"outputId":"554333fd-d91d-4348-a29b-f4373118aa28","executionInfo":{"status":"ok","timestamp":1522970080667,"user_tz":-120,"elapsed":434,"user":{"displayName":"Imad El Hanafi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"100919786130867571645"}}},"cell_type":"code","source":["out = g(x,y,z)\n","out"],"execution_count":60,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Variable containing:\n"," 0.3290\n","[torch.cuda.FloatTensor of size 1 (GPU 0)]"]},"metadata":{"tags":[]},"execution_count":60}]},{"metadata":{"id":"RmiqXhUqDZdP","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["out.backward()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"6gNIexscET0W","colab_type":"text"},"cell_type":"markdown","source":["We can now get the computed grads:"]},{"metadata":{"id":"2gFJZzpZD4kM","colab_type":"text"},"cell_type":"markdown","source":["- x.grad will contains $\\nabla_x g(x, y)$ \n","- y.grad will contains $\\nabla_y g(x, y)$ "]},{"metadata":{"id":"3Kf57RVaDuTl","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":221},"outputId":"218ed52f-f847-4608-c237-b34b468501ec","executionInfo":{"status":"ok","timestamp":1522970086974,"user_tz":-120,"elapsed":815,"user":{"displayName":"Imad El Hanafi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"100919786130867571645"}}},"cell_type":"code","source":["# it contains dg(x,y)/dx\n","x.grad"],"execution_count":62,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Variable containing:\n"," 0.3210\n"," 0.0265\n"," 0.1718\n","-0.1191\n","-0.1462\n"," 0.1195\n","-0.1124\n","-0.0202\n","-0.0497\n","-0.4000\n","[torch.cuda.FloatTensor of size 10 (GPU 0)]"]},"metadata":{"tags":[]},"execution_count":62}]},{"metadata":{"id":"fyycK73WF1vp","colab_type":"text"},"cell_type":"markdown","source":["## Non-scalar functions\n","\n","Let's consider the following example : \n","\n","$$ y = Mx $$\n","\n","the derivative of y with respect to x is $M^T$. To do so, we need to specify in the backward method a *grad_tensors* (should be the same lenght as the output)"]},{"metadata":{"id":"mLPhZhVMDvDj","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":170},"outputId":"ca8fd509-5709-4acb-8b8f-af979321e9d0","executionInfo":{"status":"ok","timestamp":1522970840251,"user_tz":-120,"elapsed":473,"user":{"displayName":"Imad El Hanafi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"100919786130867571645"}}},"cell_type":"code","source":["x = Variable(th.FloatTensor([[2,1]]).cuda(), requires_grad=True)\n","M = Variable(th.FloatTensor([[1,2],[3,4]]).cuda()) \n","y = th.mm(x,M)\n","print(\"y:\",y)\n","print(\"M :\",M)"],"execution_count":74,"outputs":[{"output_type":"stream","text":["y: Variable containing:\n"," 5  8\n","[torch.cuda.FloatTensor of size 1x2 (GPU 0)]\n","\n","M : Variable containing:\n"," 1  2\n"," 3  4\n","[torch.cuda.FloatTensor of size 2x2 (GPU 0)]\n","\n"],"name":"stdout"}]},{"metadata":{"id":"KnRnYJTiFoIe","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":204},"outputId":"1a50d6a1-e599-4c6e-c53c-1f012db2598e","executionInfo":{"status":"ok","timestamp":1522970873781,"user_tz":-120,"elapsed":508,"user":{"displayName":"Imad El Hanafi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"100919786130867571645"}}},"cell_type":"code","source":["### Do the backpro with respect to the first elemnt of y  : dy1/dx (I mean dy1/dx1 and dy1/dx2)\n","\n","y.backward(th.FloatTensor([[1, 0]]).cuda(),create_graph = True)\n","print(x.grad.data)\n","x.grad.data.zero_() #remove gradient in x.grad, or it will be accumulated\n","\n","### Do the backpro with respect to the second elemnt of y  : dy2/dx \n","\n","y.backward(th.FloatTensor([[0, 1]]).cuda(),create_graph = True)\n","print(x.grad.data)\n","x.grad.data.zero_() "],"execution_count":76,"outputs":[{"output_type":"stream","text":["\n"," 1  3\n","[torch.cuda.FloatTensor of size 1x2 (GPU 0)]\n","\n","\n"," 2  4\n","[torch.cuda.FloatTensor of size 1x2 (GPU 0)]\n","\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["\n"," 0  0\n","[torch.cuda.FloatTensor of size 1x2 (GPU 0)]"]},"metadata":{"tags":[]},"execution_count":76}]},{"metadata":{"id":"K1OAQ51IXjhQ","colab_type":"text"},"cell_type":"markdown","source":["##  Higher order derivative \n","\n","Consider the following function : $ out = x^2 + xy + y^2$\n","\n","x and y are matrices. We can get the first and second derivative easly as shown in the following cells : "]},{"metadata":{"id":"py0KPgIhFrO8","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":85},"outputId":"ca080f8f-250f-4c19-8b3e-eb71de313cfe","executionInfo":{"status":"ok","timestamp":1523013481746,"user_tz":-120,"elapsed":787,"user":{"displayName":"Imad El Hanafi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"100919786130867571645"}}},"cell_type":"code","source":["x = Variable(th.randn(2, 2), requires_grad=True)\n","y = Variable(th.randn(2, 2), requires_grad=True)\n","\n","out = x ** 2 + x*y + y ** 2\n","out"],"execution_count":34,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Variable containing:\n"," 1.1912  1.2992\n"," 1.7304  1.6375\n","[torch.FloatTensor of size 2x2]"]},"metadata":{"tags":[]},"execution_count":34}]},{"metadata":{"id":"l91SW6YnnPOk","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# do Backward\n","\n","out.backward(th.ones(2, 2), create_graph=True) # we put th.ones(2,2) to get the grad%x which is a matrix "],"execution_count":0,"outputs":[]},{"metadata":{"id":"4ewS1VyVo8ni","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":85},"outputId":"767f74ce-5fca-44c7-dcb7-e2a8416e2610","executionInfo":{"status":"ok","timestamp":1523013483721,"user_tz":-120,"elapsed":441,"user":{"displayName":"Imad El Hanafi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"100919786130867571645"}}},"cell_type":"code","source":["# x.grad will contains the grad of out % x \n","x_grad = x.grad # = 2*x+y\n","x_grad"],"execution_count":36,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Variable containing:\n","-1.5591  1.8715\n","-0.4411 -1.7144\n","[torch.FloatTensor of size 2x2]"]},"metadata":{"tags":[]},"execution_count":36}]},{"metadata":{"id":"nTCGSDPurZhe","colab_type":"text"},"cell_type":"markdown","source":["We can check that we got the right value"]},{"metadata":{"id":"2b3oDTY2o_WP","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":85},"outputId":"4ac588ae-e2f8-4b9f-c266-00cda3051091","executionInfo":{"status":"ok","timestamp":1523013485372,"user_tz":-120,"elapsed":429,"user":{"displayName":"Imad El Hanafi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"100919786130867571645"}}},"cell_type":"code","source":["2*x + y"],"execution_count":37,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Variable containing:\n","-1.5591  1.8715\n","-0.4411 -1.7144\n","[torch.FloatTensor of size 2x2]"]},"metadata":{"tags":[]},"execution_count":37}]},{"metadata":{"id":"ySp4TOPxrdyD","colab_type":"text"},"cell_type":"markdown","source":["Now using the same trick, we can get higher order derivatives. Don't forget to set the x.grad to zero, otherwise we will accumulate derivatives."]},{"metadata":{"id":"gG6hOj_HpJ61","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":85},"outputId":"ef6bfc81-4b71-46f8-d4fb-0a6fcdea623d","executionInfo":{"status":"ok","timestamp":1523013560810,"user_tz":-120,"elapsed":514,"user":{"displayName":"Imad El Hanafi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"100919786130867571645"}}},"cell_type":"code","source":["x.grad.data.zero_() # If we don't use this, we will accumulate values\n","\n","x_grad.backward(th.ones(2, 2))\n","x.grad"],"execution_count":38,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Variable containing:\n"," 2  2\n"," 2  2\n","[torch.FloatTensor of size 2x2]"]},"metadata":{"tags":[]},"execution_count":38}]},{"metadata":{"id":"6GtDFd2Wp6dl","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}